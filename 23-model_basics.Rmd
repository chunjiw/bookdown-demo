# Model Basics

Author: Ron 
Reviewer:

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tibble)
library(ggplot2)
library(dplyr)
```



### 23.2.1 Exercises {-}
1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
```
Run linear model and visualize it
```{r}
fit <- lm(y~x, data = sim1a)
ggplot(sim1a,aes(x,y))+
  geom_point(size = 2, color = "grey30")+
  geom_abline(intercept = fit$coefficients[1],slope = fit$coefficients[2])

```
Sometimes, one single abnormal value forces the fitted line deviate from the "intutively" best lines.

2.One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2]
}

measure_distance <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  mean(abs(diff))
}

measure_distance2 <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}
```
Compare the two measures of distance
```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
best1 <- optim(c(0,0), measure_distance, data = sim1a)
best2 <- optim(c(0,0), measure_distance2, data = sim1a)

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best1$par[1], slope = best1$par[2], color = "red")+
  geom_abline(intercept = best2$par[1],slope = best2$par[2], color = "cyan")
```
When there are many abnormal points, the `cyan` line will perform better using absolute distances. It is better because measn-square-distance tends to overemphasize abnormal values.
3. One challenge with performing numerical optimisation is that it’s only guaranteed to find one local optima. What’s the problem with optimising a three parameter model like this?

```{r}
model1 <- function(a, data) {
  a[1] + data$x * a[2] + a[3]
}

measure_distance3 <- function(mod, data) {
  diff <- data$y - model1(mod, data)
  sqrt(mean(diff ^ 2))
}

sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
)
best3 <- optim(c(0,0,0), measure_distance3, data = sim1a)
best3$par

ggplot(sim1a, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(intercept = best3$par[1] + best3$par[3], slope = best3$par[2], color = "red")
```
There are essentially infinitely many solutions since the model we built contains redudant information in `a[1]` and `a[3]`.

### 23.3.3 Exercises {-}

1.Instead of using `lm()` to fit a straight line, you can use `loess()` to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using `loess()` instead of `lm()`. How does the result compare to `geom_smooth()`?

```{r}
fit1 <- lm(y~x, data = sim1)
fit2 <- loess(y~x, data = sim1,degree = 2)

grid <- sim1 %>% data_grid(x)
grid1 <- grid %>%
  add_predictions(fit1)
sim1_1 <- sim1 %>% add_residuals(fit1)

grid2 <- grid %>% 
  add_predictions(fit2)
sim1_2 <- sim1 %>% add_residuals(fit2)
```
plot the predictions
```{r}
ggplot(sim1,aes(x=x))+
  geom_point(aes(y=y))+
  geom_line(data = grid1, aes(y = pred), color = 'red')+
  geom_smooth(data = grid2, aes(y = pred),color = 'cyan')
```
Plot the residuals
```{r}
ggplot() + 
  geom_freqpoly(data = sim1_1, aes(resid),binwidth = 0.5,color = 'red') +
  geom_freqpoly(data = sim1_2, aes(resid),binwidth = 0.5, color = 'cyan')
```
2. `add_predictions()` is paired with `gather_predictions()` and `spread_predictions()`. How do these three functions differ?

Similar to the idea of `gather` and `spread` in the `tidyr` package. `spread_predicitions` will create a *fat* table with each model creating a column of its own prediction. `gather_predictions` will create two columns with one columns indicating the type of the model and another one it prediction, resulting in a *tall* table.

3. What does `geom_ref_line()` do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

`geom_ref_line()` add a reference line in the graph, it comes from `modelr` package. It is useful for you to detect the trend and disribution of residuals visually.

4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

```{r}
ggplot() + 
  geom_freqpoly(data = sim1_1, aes(abs(resid)),binwidth = 0.5,color = 'red')
```

You want to check the absolute residuals because it helps to see the overall quality of the prediction but it won't give you the hint about the distribution of residuals with respect to *0*. For example, it may be possible that there is only one large possitve residual and many small negative ones.